{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369b5917",
   "metadata": {},
   "source": [
    "# Create & Run a Local RAG Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3496144",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "The goal of RAG is to take information and pass it to an LLM, so it can generate outputs based on that information.\n",
    "\n",
    "* Retrieval - Find relevant information given a query, e.g. \"What are the macronutrients & what do they do?\" -> retrieves passages of text related to the macronutrients from a nutrition textbook.\n",
    "\n",
    "* Augmented - We want to take the relevant information & augment our input (prompt) to an LLM with that relevant information.\n",
    "\n",
    "* Generation - Take the first 2 steps & pass them to an LLM for generative outputs.\n",
    "\n",
    "Where RAG came from - Facebook / Meta AI Paper: *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*\n",
    "> This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4675e",
   "metadata": {},
   "source": [
    "## Why RAG?\n",
    "\n",
    "The main goal of RAG is to improve the generation outputs of LLMs.\n",
    "\n",
    "1. Prevent hallucinations - LLMs are incredibly good at generating good *looking* text, however, this text doesn't mean that it is factual. RAG can help LLMs generate information based on relevant passages that are factual.\n",
    "\n",
    "2. Work with custom data - Many base LLMs are trained with internet-scale data. This means they have a fairly good understanding of language in general. However, it also does a lot of their responses can be generic in nature. RAG helps to create specific responses based on specific documents (e.g. your own companies customer support documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591eb51",
   "metadata": {},
   "source": [
    "## What can RAG be used for?\n",
    "\n",
    "* Customer Support Q&A Chat - Treat your existing customer support documents as a resource and when a customer asks a question, you could have a retrieval system, retrieve relevant documentation snippets & then have a LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\".\n",
    "\n",
    "* Email Chain Analysis - Let's say you are a large insurance company & you have chains and chains of emails of customer claims. You could use a RAG pipeline to find relevant information from those emails & then use an LLM to process that information into structured data.\n",
    "\n",
    "* Company Interval Documentation Chat\n",
    "\n",
    "* Textbook Q&A - Let's say you are a nutrition student and you've got a 1200 pages textbook read, you could build a RAG pipeline to go through the textbook and find relevant passages to the questions you have.\n",
    "\n",
    "Common theme here: Take your relevant documents to a query & process them with an LLM.\n",
    "\n",
    "From this angle, consider LLM as a calculator for words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181070c",
   "metadata": {},
   "source": [
    "## Why Local?\n",
    "\n",
    "Fun. \n",
    "\n",
    "Privacy, Speed, Cost.\n",
    "\n",
    "* Privacy - If you have private documentation, maybe you don't want to send that to an API. You want to setup an LLM and run it on your own hardware.\n",
    "* Speed - Whenever you use an API, you have to send some kind of data across the internet. This takes time. Running locally means we don't have to wait for transfers of data.\n",
    "* Cost - If you own your hardware, the cost is paid. It may have a large cost to begin with. But overtime, you don't have to keep paying API fees.\n",
    "* No Vendor Lock-in - If you run your own software/ hardware. If Large company shuts down tomorrow, you can still run your business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5098a",
   "metadata": {},
   "source": [
    "## What Will Be Built?\n",
    "\n",
    "Build NutriChat to \"chat with a nutrition document\".\n",
    "\n",
    "Specifically:\n",
    "\n",
    "1. Open a PDF document (you could use almost any PDF here or even a collection of PDFs).\n",
    "2. Format the text of the PDF textbook ready for an embedding model.\n",
    "3. Embbed all of the chunks of text in the textbook, and turn them into numerical representations (embeddings) which can store for later.\n",
    "4. Build a retrieval system that uses vector search to find relevant chunk of text based on a query.\n",
    "5. Create a prompt that incorporates the retrieved pieces of text.\n",
    "6. Generate an answer to a query based on the passages of the textbook with an LLM.\n",
    "\n",
    "All Locally!\n",
    "\n",
    "1. Steps 1 - 3: Document Preprocessing & Embedding Creation.\n",
    "2. Steps 4 - 6: Search & Answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb01dd",
   "metadata": {},
   "source": [
    "## 1. Document / Text Preprocessing & Embedding Creation\n",
    "\n",
    "Ingredients:\n",
    "* PDF document of choice (note: this could be almost any kind of document, just that PDFs are focused for now).\n",
    "* Embedding model of choice\n",
    "\n",
    "Steps:\n",
    "1. Import PDF Document.\n",
    "2. Preprocess Text for Embedding (e.g. Split into Chunks of Sentences).\n",
    "3. Embbed Text Chunks with Embedding Model.\n",
    "4. Save Embeddings to File for Later (Embeddings will store on files for many years or until you lose your hard drive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832a118",
   "metadata": {},
   "source": [
    "## Import PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b390d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ab4cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] File human-nutrition-text.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "# path to document\n",
    "pdf_path = 'human-nutrition-text.pdf'\n",
    "\n",
    "# download PDF\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f'[INFO] File does not exist, downloading...')\n",
    "    \n",
    "    # url of the pdf\n",
    "    url = 'https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf'\n",
    "    \n",
    "    # the local file name to save the downloaded file\n",
    "    fname = pdf_path\n",
    "    \n",
    "    # GET request\n",
    "    res = requests.get(url)\n",
    "    \n",
    "    # check if the request is successful\n",
    "    if res.status_code == 200:\n",
    "        # open the file & save it\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(res.content)\n",
    "        print(f'[INFO] The file has been downloaded & saved as {fname}.')\n",
    "    else:\n",
    "        print(f'[INFO] Failed to download the file. Status Code: {res.status_code}')\n",
    "else:\n",
    "    print(f'[INFO] File {pdf_path} exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6e0be",
   "metadata": {},
   "source": [
    "PDF is now available, let's open it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7aa0411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc679c8b1ca48c990c72808edbdbfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_no': -41,\n",
       "  'page_char_cnt': 29,\n",
       "  'page_word_cnt': 4,\n",
       "  'page_sentence_cnt_raw': 1,\n",
       "  'page_token_cnt': 7.25,\n",
       "  'text': 'Human Nutrition: 2020 Edition'},\n",
       " {'page_no': -40,\n",
       "  'page_char_cnt': 0,\n",
       "  'page_word_cnt': 1,\n",
       "  'page_sentence_cnt_raw': 1,\n",
       "  'page_token_cnt': 0.0,\n",
       "  'text': ''}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz # from PyMuPDF\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    '''Performs minor formatting on text.'''\n",
    "    cleaned_text = text.replace('\\n', ' ').strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(path: str) -> list[dict]:\n",
    "    doc = fitz.open(path)\n",
    "    pages_and_texts = []\n",
    "    \n",
    "    for page_no, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        pages_and_texts.append({'page_no': page_no - 41,\n",
    "                                'page_char_cnt': len(text),\n",
    "                                'page_word_cnt': len(text.split(' ')),\n",
    "                                'page_sentence_cnt_raw': len(text.split('. ')),\n",
    "                                'page_token_cnt': len(text) / 4, # 1 token ~ 4 chars\n",
    "                                'text': text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c304778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': 60,\n",
       "  'page_char_cnt': 1013,\n",
       "  'page_word_cnt': 168,\n",
       "  'page_sentence_cnt_raw': 6,\n",
       "  'page_token_cnt': 253.25,\n",
       "  'text': 'all other organ systems in the human body. We will learn the  process of nutrient digestion and absorption, which further  reiterates the importance of developing a healthy diet to maintain  a healthier you. The evidence abounds that food can indeed be “thy  medicine.”  Learning Activities  Technology Note: The second edition of the Human  Nutrition Open Educational Resource (OER) textbook  features interactive learning activities.\\xa0 These activities are  available in the web-based textbook and not available in the  downloadable versions (EPUB, Digital PDF, Print_PDF, or  Open Document).  Learning activities may be used across various mobile  devices, however, for the best user experience it is strongly  recommended that users complete these activities using a  desktop or laptop computer and in Google Chrome.  \\xa0 An interactive or media element has been  excluded from this version of the text. You can  view it online here:  http:/ /pressbooks.oer.hawaii.edu/ humannutrition2/?p=71  60  |  Introduction'},\n",
       " {'page_no': 999,\n",
       "  'page_char_cnt': 2033,\n",
       "  'page_word_cnt': 320,\n",
       "  'page_sentence_cnt_raw': 25,\n",
       "  'page_token_cnt': 508.25,\n",
       "  'text': 'The bacterium Listeria monocytogenes is found in soft cheeses,  unpasteurized milk, meat, and seafood. It causes a disease called  listeriosis that can bring about fever, headache, nausea, and  vomiting. Listeria monocytogenes mostly affects pregnant women,  newborns, older adults, and people with cancer and compromised  immune systems.  The food infection by Escherichia coli is found in raw or  undercooked meat, raw vegetables, unpasteurized milk, minimally  processed ciders and juices, and contaminated drinking water.  Symptoms can occur a few days after eating, and include watery and  bloody diarrhea, severe stomach cramps, and dehydration. More  severe complications may include colitis, neurological symptoms,  stroke, and hemolytic uremic syndrome. In young children, an E.  coli infection can cause kidney failure and death.  The bacterium Clostridium botulinum causes botulism. Sources  include improperly canned foods, lunch meats, and garlic. An  infected person may experience symptoms within four to thirty- six hours after eating. Symptoms could include nerve dysfunction,  such as double vision, inability to swallow, speech difficulty, and  progressive paralysis of the respiratory system. Botulism can also be  fatal.  Campylobacter jejuni causes the disease campylobacteriosis. It  is the most commonly identified bacterial cause of diarrhea  worldwide. Consuming undercooked chicken, or food contaminated  with the juices of raw chicken, is the most frequent source of this  infection. Other sources include raw meat and unpasteurized milk.  Within two to five days after consumption, symptoms can begin  and include diarrhea, stomach cramps, fever, and bloody stools. The  duration of this disease is about seven to ten days.  The food infection shigellosis is caused by Shigella, of which there  are several types. Sources include undercooked liquid or moist food  Prevention. http:/ /www.cdc.gov/salmonella/. Updated  January 24, 2018. Accessed January 29, 2018.  The Causes of Food Contamination  |  999'},\n",
       " {'page_no': 591,\n",
       "  'page_char_cnt': 1538,\n",
       "  'page_word_cnt': 265,\n",
       "  'page_sentence_cnt_raw': 11,\n",
       "  'page_token_cnt': 384.5,\n",
       "  'text': 'car. Does it drive faster with a half-tank of gas or a full one? It does  not matter; the car drives just as fast as long as it has gas. Similarly,  depletion of B vitamins will cause problems in energy metabolism,  but having more than is required to run metabolism does not speed  it up. Buyers of B-vitamin supplements beware; B vitamins are not  stored in the body and all excess will be flushed down the toilet  along with the extra money spent.  B vitamins are naturally present in numerous foods, and many  other foods are enriched with them. In the United States, B-vitamin  deficiencies are rare; however in the nineteenth century some  vitamin-B deficiencies plagued many people in North America.  Niacin deficiency, also known as pellagra, was prominent in poorer  Americans whose main dietary staple was refined cornmeal. Its  symptoms were severe and included diarrhea, dermatitis, dementia,  and even death. Some of the health consequences of pellagra are  the result of niacin being in insufficient supply to support the body’s  metabolic functions.  Learning Activities  Technology Note: The second edition of the Human  Nutrition Open Educational Resource (OER) textbook  features interactive learning activities.\\xa0 These activities are  available in the web-based textbook and not available in the  downloadable versions (EPUB, Digital PDF, Print_PDF, or  Open Document).  Learning activities may be used across various mobile  devices, however, for the best user experience it is strongly  Water-Soluble Vitamins  |  591'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f1d7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_char_cnt</th>\n",
       "      <th>page_word_cnt</th>\n",
       "      <th>page_sentence_cnt_raw</th>\n",
       "      <th>page_token_cnt</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>320</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Human Nutrition: 2020  Edition  UNIVERSITY OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>212</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>797</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "      <td>199.25</td>\n",
       "      <td>Contents  Preface  University of Hawai‘i at Mā...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_no  page_char_cnt  page_word_cnt  page_sentence_cnt_raw  \\\n",
       "0      -41             29              4                      1   \n",
       "1      -40              0              1                      1   \n",
       "2      -39            320             54                      1   \n",
       "3      -38            212             32                      1   \n",
       "4      -37            797            145                      2   \n",
       "\n",
       "   page_token_cnt                                               text  \n",
       "0            7.25                      Human Nutrition: 2020 Edition  \n",
       "1            0.00                                                     \n",
       "2           80.00  Human Nutrition: 2020  Edition  UNIVERSITY OF ...  \n",
       "3           53.00  Human Nutrition: 2020 Edition by University of...  \n",
       "4          199.25  Contents  Preface  University of Hawai‘i at Mā...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226ce042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_char_cnt</th>\n",
       "      <th>page_word_cnt</th>\n",
       "      <th>page_sentence_cnt_raw</th>\n",
       "      <th>page_token_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.59</td>\n",
       "      <td>198.89</td>\n",
       "      <td>9.97</td>\n",
       "      <td>287.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.44</td>\n",
       "      <td>95.75</td>\n",
       "      <td>6.19</td>\n",
       "      <td>140.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.75</td>\n",
       "      <td>134.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>190.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1232.50</td>\n",
       "      <td>215.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>308.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1605.25</td>\n",
       "      <td>271.25</td>\n",
       "      <td>14.00</td>\n",
       "      <td>401.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>577.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_no  page_char_cnt  page_word_cnt  page_sentence_cnt_raw  \\\n",
       "count  1208.00        1208.00        1208.00                1208.00   \n",
       "mean    562.50        1148.59         198.89                   9.97   \n",
       "std     348.86         560.44          95.75                   6.19   \n",
       "min     -41.00           0.00           1.00                   1.00   \n",
       "25%     260.75         762.75         134.00                   4.00   \n",
       "50%     562.50        1232.50         215.00                  10.00   \n",
       "75%     864.25        1605.25         271.25                  14.00   \n",
       "max    1166.00        2308.00         429.00                  32.00   \n",
       "\n",
       "       page_token_cnt  \n",
       "count         1208.00  \n",
       "mean           287.15  \n",
       "std            140.11  \n",
       "min              0.00  \n",
       "25%            190.69  \n",
       "50%            308.12  \n",
       "75%            401.31  \n",
       "max            577.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa7d67",
   "metadata": {},
   "source": [
    "Why would we care about token count?\n",
    "\n",
    "Token count is important to think about because:\n",
    "\n",
    "1. Embedding models don't deal with infinite tokens.\n",
    "2. LLMs don't deal with infinite tokens.\n",
    "\n",
    "For example, an embedding model may have been trained to embbed sequences of 384 tokens into numerical space (sentence-transformers `all-mpnet-base-v2`, see: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)\n",
    "\n",
    "As for LLMs, they can't accept infinite tokens in their context window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9d8db",
   "metadata": {},
   "source": [
    "## Further Text Preprocessing\n",
    "\n",
    "Splitting pages into sentences.\n",
    "\n",
    "2 Ways to do this:\n",
    "\n",
    "1. Done this by splitting on `'.'`.\n",
    "2. Do this by NLP library, such as spaCy and nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d853f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This is another sentence., I like elephants.]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline (turning texts into sentences)\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "# Create document instance as an example\n",
    "doc = nlp('This is a sentence. This is another sentence. I like elephants.')\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "# Print sentences split\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b50c81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_no': -41,\n",
       " 'page_char_cnt': 29,\n",
       " 'page_word_cnt': 4,\n",
       " 'page_sentence_cnt_raw': 1,\n",
       " 'page_token_cnt': 7.25,\n",
       " 'text': 'Human Nutrition: 2020 Edition'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aff9f8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4014c574274542c0925f0f2be0aa3f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item['sentences'] = list(nlp(item['text']).sents)\n",
    "\n",
    "    # Make sure all sentences are string (default type is spaCy data type)\n",
    "    item['sentences'] = [str(sentence) for sentence in item['sentences']]\n",
    "    \n",
    "    # Count the sentences\n",
    "    item['page_sentence_cnt_spacy'] = len(item['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c288e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': 1166,\n",
       "  'page_char_cnt': 257,\n",
       "  'page_word_cnt': 44,\n",
       "  'page_sentence_cnt_raw': 3,\n",
       "  'page_token_cnt': 64.25,\n",
       "  'text': '23. Vitamin D reused “The Functions of Vitamin D” by Allison  Calabrese / Attribution – Sharealike  24. Vitamin K reused “Kale Lacinato Lacinato Kale” by BlackRiv\\xa0/  Pixabay License; “Phylloquinone structure” by Mysid\\xa0/ Public  Domain  1166  |  Attributions',\n",
       "  'sentences': ['23.',\n",
       "   'Vitamin D reused “The Functions of Vitamin D” by Allison  Calabrese / Attribution – Sharealike  24.',\n",
       "   'Vitamin K reused “Kale Lacinato Lacinato Kale” by BlackRiv\\xa0/  Pixabay License; “Phylloquinone structure” by Mysid\\xa0/ Public  Domain  1166  |  Attributions'],\n",
       "  'page_sentence_cnt_spacy': 3}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "241b4946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_char_cnt</th>\n",
       "      <th>page_word_cnt</th>\n",
       "      <th>page_sentence_cnt_raw</th>\n",
       "      <th>page_token_cnt</th>\n",
       "      <th>page_sentence_cnt_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.59</td>\n",
       "      <td>198.89</td>\n",
       "      <td>9.97</td>\n",
       "      <td>287.15</td>\n",
       "      <td>10.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.44</td>\n",
       "      <td>95.75</td>\n",
       "      <td>6.19</td>\n",
       "      <td>140.11</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.75</td>\n",
       "      <td>134.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>190.69</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1232.50</td>\n",
       "      <td>215.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>308.12</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1605.25</td>\n",
       "      <td>271.25</td>\n",
       "      <td>14.00</td>\n",
       "      <td>401.31</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_no  page_char_cnt  page_word_cnt  page_sentence_cnt_raw  \\\n",
       "count  1208.00        1208.00        1208.00                1208.00   \n",
       "mean    562.50        1148.59         198.89                   9.97   \n",
       "std     348.86         560.44          95.75                   6.19   \n",
       "min     -41.00           0.00           1.00                   1.00   \n",
       "25%     260.75         762.75         134.00                   4.00   \n",
       "50%     562.50        1232.50         215.00                  10.00   \n",
       "75%     864.25        1605.25         271.25                  14.00   \n",
       "max    1166.00        2308.00         429.00                  32.00   \n",
       "\n",
       "       page_token_cnt  page_sentence_cnt_spacy  \n",
       "count         1208.00                  1208.00  \n",
       "mean           287.15                    10.32  \n",
       "std            140.11                     6.30  \n",
       "min              0.00                     0.00  \n",
       "25%            190.69                     5.00  \n",
       "50%            308.12                    10.00  \n",
       "75%            401.31                    15.00  \n",
       "max            577.00                    28.00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d5bcd",
   "metadata": {},
   "source": [
    "### Chunking our sentences together\n",
    "\n",
    "The concept of splitting larger pieces of texts into smaller ones, often refer to as `text splitting` or `chunking`.\n",
    "\n",
    "There is no 100% of correct way to do this - experiment!\n",
    "\n",
    "To keep it simple, it will split into groups of 10 sentences.\n",
    "\n",
    "There are frameworks such as `langchain` which can help with this, but we will use `python` for now.\n",
    "- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n",
    "\n",
    "Why do we do this:\n",
    "1. So the texts are easier to filter (smaller group of texts can be easier to inspect than large passages of texts).\n",
    "2. So the text chunks can fit into the embedding model of context. (eg. 384 tokens has a limit).\n",
    "3. So the contexts passed into LLM can be more specific and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c248b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "# Create a function to split lists of texts recurively into chunk size\n",
    "# eg. [20] -> [10, 10] or [25] -> [10, 10, 5]\n",
    "def split_list(input_list: list[str],\n",
    "               split_size: int=num_sentence_chunk_size) -> list[list[str]]:\n",
    "    return [input_list[i : i + split_size] for i in range(0, len(input_list), split_size)]\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "095cdb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6d03ae07c843169673f6f3709dd52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item['sentence_chunks'] = split_list(input_list=item['sentences'],\n",
    "                                         split_size=num_sentence_chunk_size)\n",
    "    item['num_chunks'] = len(item['sentence_chunks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8222b054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': 400,\n",
       "  'page_char_cnt': 2070,\n",
       "  'page_word_cnt': 357,\n",
       "  'page_sentence_cnt_raw': 15,\n",
       "  'page_token_cnt': 517.5,\n",
       "  'text': 'as those that derive more than 30 percent of calories from protein.  Many people follow high-protein diets because marketers tout  protein’s ability to stimulate weight loss. It is true that following  high-protein diets increases weight loss in some people. However  the number of individuals that remain on this type of diet is low  and many people who try the diet and stop regain the weight they  had lost. Additionally, there is a scientific hypothesis that there may  be health consequences of remaining on high-protein diets for the  long-term, but clinical trials are ongoing or scheduled to examine  this hypothesis further. As the high-protein diet trend arose so  did the intensely debated issue of whether there are any health  consequences of eating too much protein. Observational studies  conducted in the general population suggest diets high in animal  protein, specifically those in which the primary protein source is  red meat, are linked to a higher risk for kidney stones, kidney  disease, liver malfunction, colorectal cancer, and osteoporosis.  However, diets that include lots of red meat are also high in  saturated fat and cholesterol and sometimes linked to unhealthy  lifestyles, so it is difficult to conclude that the high protein content  is the culprit.  High protein diets appear to only increase the progression of  kidney disease and liver malfunction in people who already have  kidney or liver malfunction, and not to cause these problems.  However, the prevalence of kidney disorders is relatively high and  underdiagnosed. In regard to colon cancer, an assessment of more  than ten studies performed around the world published in the June  2011 issue of PLoS purports that a high intake of red meat and  processed meat is associated with a significant increase in colon  cancer risk.1Although there are a few ideas, the exact mechanism of  1.\\xa0Chan DS, Lau R, et al. (2011). Red and Processed Meat and  Colorectal Cancer Incidence: Meta-Analysis of  Prospective Studies. PLoS One,\\xa06(6), e20456.  400  |  Diseases Involving Proteins',\n",
       "  'sentences': ['as those that derive more than 30 percent of calories from protein.',\n",
       "   ' Many people follow high-protein diets because marketers tout  protein’s ability to stimulate weight loss.',\n",
       "   'It is true that following  high-protein diets increases weight loss in some people.',\n",
       "   'However  the number of individuals that remain on this type of diet is low  and many people who try the diet and stop regain the weight they  had lost.',\n",
       "   'Additionally, there is a scientific hypothesis that there may  be health consequences of remaining on high-protein diets for the  long-term, but clinical trials are ongoing or scheduled to examine  this hypothesis further.',\n",
       "   'As the high-protein diet trend arose so  did the intensely debated issue of whether there are any health  consequences of eating too much protein.',\n",
       "   'Observational studies  conducted in the general population suggest diets high in animal  protein, specifically those in which the primary protein source is  red meat, are linked to a higher risk for kidney stones, kidney  disease, liver malfunction, colorectal cancer, and osteoporosis.',\n",
       "   ' However, diets that include lots of red meat are also high in  saturated fat and cholesterol and sometimes linked to unhealthy  lifestyles, so it is difficult to conclude that the high protein content  is the culprit.',\n",
       "   ' High protein diets appear to only increase the progression of  kidney disease and liver malfunction in people who already have  kidney or liver malfunction, and not to cause these problems.',\n",
       "   ' However, the prevalence of kidney disorders is relatively high and  underdiagnosed.',\n",
       "   'In regard to colon cancer, an assessment of more  than ten studies performed around the world published in the June  2011 issue of PLoS purports that a high intake of red meat and  processed meat is associated with a significant increase in colon  cancer risk.1Although there are a few ideas, the exact mechanism of  1.',\n",
       "   '\\xa0Chan DS, Lau R, et al. (',\n",
       "   '2011).',\n",
       "   'Red and Processed Meat and  Colorectal Cancer Incidence: Meta-Analysis of  Prospective Studies.',\n",
       "   'PLoS One,\\xa06(6), e20456.',\n",
       "   ' 400  |  Diseases Involving Proteins'],\n",
       "  'page_sentence_cnt_spacy': 16,\n",
       "  'sentence_chunks': [['as those that derive more than 30 percent of calories from protein.',\n",
       "    ' Many people follow high-protein diets because marketers tout  protein’s ability to stimulate weight loss.',\n",
       "    'It is true that following  high-protein diets increases weight loss in some people.',\n",
       "    'However  the number of individuals that remain on this type of diet is low  and many people who try the diet and stop regain the weight they  had lost.',\n",
       "    'Additionally, there is a scientific hypothesis that there may  be health consequences of remaining on high-protein diets for the  long-term, but clinical trials are ongoing or scheduled to examine  this hypothesis further.',\n",
       "    'As the high-protein diet trend arose so  did the intensely debated issue of whether there are any health  consequences of eating too much protein.',\n",
       "    'Observational studies  conducted in the general population suggest diets high in animal  protein, specifically those in which the primary protein source is  red meat, are linked to a higher risk for kidney stones, kidney  disease, liver malfunction, colorectal cancer, and osteoporosis.',\n",
       "    ' However, diets that include lots of red meat are also high in  saturated fat and cholesterol and sometimes linked to unhealthy  lifestyles, so it is difficult to conclude that the high protein content  is the culprit.',\n",
       "    ' High protein diets appear to only increase the progression of  kidney disease and liver malfunction in people who already have  kidney or liver malfunction, and not to cause these problems.',\n",
       "    ' However, the prevalence of kidney disorders is relatively high and  underdiagnosed.'],\n",
       "   ['In regard to colon cancer, an assessment of more  than ten studies performed around the world published in the June  2011 issue of PLoS purports that a high intake of red meat and  processed meat is associated with a significant increase in colon  cancer risk.1Although there are a few ideas, the exact mechanism of  1.',\n",
       "    '\\xa0Chan DS, Lau R, et al. (',\n",
       "    '2011).',\n",
       "    'Red and Processed Meat and  Colorectal Cancer Incidence: Meta-Analysis of  Prospective Studies.',\n",
       "    'PLoS One,\\xa06(6), e20456.',\n",
       "    ' 400  |  Diseases Involving Proteins']],\n",
       "  'num_chunks': 2}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f44611b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>page_char_cnt</th>\n",
       "      <th>page_word_cnt</th>\n",
       "      <th>page_sentence_cnt_raw</th>\n",
       "      <th>page_token_cnt</th>\n",
       "      <th>page_sentence_cnt_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.59</td>\n",
       "      <td>198.89</td>\n",
       "      <td>9.97</td>\n",
       "      <td>287.15</td>\n",
       "      <td>10.32</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.44</td>\n",
       "      <td>95.75</td>\n",
       "      <td>6.19</td>\n",
       "      <td>140.11</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.75</td>\n",
       "      <td>134.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>190.69</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1232.50</td>\n",
       "      <td>215.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>308.12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1605.25</td>\n",
       "      <td>271.25</td>\n",
       "      <td>14.00</td>\n",
       "      <td>401.31</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_no  page_char_cnt  page_word_cnt  page_sentence_cnt_raw  \\\n",
       "count  1208.00        1208.00        1208.00                1208.00   \n",
       "mean    562.50        1148.59         198.89                   9.97   \n",
       "std     348.86         560.44          95.75                   6.19   \n",
       "min     -41.00           0.00           1.00                   1.00   \n",
       "25%     260.75         762.75         134.00                   4.00   \n",
       "50%     562.50        1232.50         215.00                  10.00   \n",
       "75%     864.25        1605.25         271.25                  14.00   \n",
       "max    1166.00        2308.00         429.00                  32.00   \n",
       "\n",
       "       page_token_cnt  page_sentence_cnt_spacy  num_chunks  \n",
       "count         1208.00                  1208.00     1208.00  \n",
       "mean           287.15                    10.32        1.53  \n",
       "std            140.11                     6.30        0.64  \n",
       "min              0.00                     0.00        0.00  \n",
       "25%            190.69                     5.00        1.00  \n",
       "50%            308.12                    10.00        1.00  \n",
       "75%            401.31                    15.00        2.00  \n",
       "max            577.00                    28.00        3.00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19bad1",
   "metadata": {},
   "source": [
    "### Splitting each chunk into its own item\n",
    "\n",
    "We'd like to embbed each chunk of sentences into its own numerical representation.\n",
    "\n",
    "That'll give us a good level of granularity.\n",
    "\n",
    "Meaning, we can dive specifically into text sample that was used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aed4cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5383a0e3c04c4656a907b7e5d41f5c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1843"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item['sentence_chunks']:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict['page_no'] = item['page_no']\n",
    "        \n",
    "        # Join the sentences together into paragraph-like structure, aka join the list of sentences into one paragraph\n",
    "        joined_sentence_chunk = ''.join(sentence_chunk).replace('  ', ' ').strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # '.A' -> '. A'\n",
    "        \n",
    "        chunk_dict['sentence_chunk'] = joined_sentence_chunk\n",
    "        \n",
    "        # get some stats on the chunks\n",
    "        chunk_dict['chunk_char_count'] = len(joined_sentence_chunk)\n",
    "        chunk_dict['chunk_word_count'] = len([word for word in joined_sentence_chunk.split(' ')])\n",
    "        chunk_dict['chunk_token_count'] = len(joined_sentence_chunk) / 4 # 1 token = ~4 chars\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "        \n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d99dca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': 434,\n",
       "  'sentence_chunk': 'and the impairment and ability to perform certain activities, as in driving a car. As a general rule, the liver can metabolize one standard drink (defined as 12 ounces of beer, 5 ounces of wine, or 1 ½ ounces of hard liquor) per hour. Drinking more than this, or more quickly, will cause BAC to rise to potentially unsafe levels. Table 10.1 “Mental and Physical Effects of Different BAC Levels” summarizes the mental and physical effects associated with different BAC levels. \\xa0 Table 7.1 Mental and Physical Effects of Different BAC Levels BAC Percent Typical Effects 0.02 Some loss of judgment, altered mood, relaxation, increased body warmth 0.05 Exaggerated behavior, impaired judgment, may have some loss of muscle control (focusing eyes), usually good feeling, lowered alertness, release of inhibition 0.08 Poor muscle coordination (balance, speech, vision, reaction time), difficulty detecting danger, and impaired judgment, self-control, reasoning, and memory 0.10 Clear deterioration of muscle control and reaction time, slurred speech, poor coordination, slowed thinking 0.15 Far less muscle control than normal, major loss of balance, vomiting In addition to the one drink per hour guideline, the rate at which an individual’s BAC rises is affected by the following factors: • Sex (A woman’s BAC will rise more quickly than a man’s.) • Weight (BAC will rise more slowly for heavier people.) • Genetics • Length of time as a heavy drinker • Type of alcohol consumed • Amount of alcohol consumed • Consumption rate • Consumption before or after a meal (food in the stomach slows absorption) 434 | Introduction',\n",
       "  'chunk_char_count': 1617,\n",
       "  'chunk_word_count': 259,\n",
       "  'chunk_token_count': 404.25}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8245dd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>583.38</td>\n",
       "      <td>734.83</td>\n",
       "      <td>112.72</td>\n",
       "      <td>183.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>347.79</td>\n",
       "      <td>447.43</td>\n",
       "      <td>71.07</td>\n",
       "      <td>111.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>280.50</td>\n",
       "      <td>315.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.00</td>\n",
       "      <td>746.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>186.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>890.00</td>\n",
       "      <td>1118.50</td>\n",
       "      <td>173.00</td>\n",
       "      <td>279.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>1831.00</td>\n",
       "      <td>297.00</td>\n",
       "      <td>457.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_no  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count  1843.00           1843.00           1843.00            1843.00\n",
       "mean    583.38            734.83            112.72             183.71\n",
       "std     347.79            447.43             71.07             111.86\n",
       "min     -41.00             12.00              3.00               3.00\n",
       "25%     280.50            315.00             45.00              78.75\n",
       "50%     586.00            746.00            114.00             186.50\n",
       "75%     890.00           1118.50            173.00             279.62\n",
       "max    1166.00           1831.00            297.00             457.75"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427f6f1",
   "metadata": {},
   "source": [
    "### Filter chunks of texts for short chunks\n",
    "\n",
    "These chunks may not contain much useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "758a5f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 27.75 | Text: In exchange, for the reabsorption of sodium and water, potassium is excreted. Regulation of Water Balance | 169\n",
      "Chunk token count: 12.75 | Text: PART VI CHAPTER 6. PROTEIN Chapter 6. Protein | 357\n",
      "Chunk token count: 15.25 | Text: Accessed November 30, 2017. Discovering Nutrition Facts | 737\n",
      "Chunk token count: 17.75 | Text: Table 6.1 Essential and Nonessential Amino Acids Defining Protein | 365\n",
      "Chunk token count: 16.25 | Text: Table 14.2  Micronutrient Levels during Puberty 886 | Adolescence\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_len = 30\n",
    "\n",
    "for row in df[df['chunk_token_count'] <= min_token_len].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f594c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': -39,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE',\n",
       "  'chunk_char_count': 308,\n",
       "  'chunk_word_count': 42,\n",
       "  'chunk_token_count': 77.0},\n",
       " {'page_no': -38,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition by University of Hawai‘i at Mānoa Food Science and Human Nutrition Program is licensed under a Creative Commons Attribution 4.0 International License, except where otherwise noted.',\n",
       "  'chunk_char_count': 210,\n",
       "  'chunk_word_count': 30,\n",
       "  'chunk_token_count': 52.5}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter DataFrame for rows with under 30 tokens\n",
    "pages_and_chunks_over_min_token_len = df[df['chunk_token_count'] > min_token_len].to_dict(orient='records')\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a976247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': 1127,\n",
       "  'sentence_chunk': 'Donini LM, Marsili D, Graziani MP, Imbriale M, Cannella C. (2004). Orthorexia nervosa: a preliminary study with a proposal for diagnosis and an attempt to measure the dimension of the phenomenon. Eating and Weight Disorders,\\xa09(2), 151‐157. 9.\\xa0Orthorexia. (2017, February 26). National Eating Disorders Association. https:/ /www.nationaleatingdisorders.org/learn/by- eating-disorder/other/orthorexia 10.\\xa0Mathieu J. (2005). What is orthorexia?',\n",
       "  'chunk_char_count': 441,\n",
       "  'chunk_word_count': 53,\n",
       "  'chunk_token_count': 110.25}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks_over_min_token_len, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9707683",
   "metadata": {},
   "source": [
    "### Embedding Text Chunks\n",
    "\n",
    "Embeddings are a broad but powerful concept.\n",
    "\n",
    "While humans understand text, machines understand numbers.\n",
    "\n",
    "What we'd like to do:\n",
    "- Turn our text chunks into numbers, specifically embeddings.\n",
    "\n",
    "A useful numerical representation.\n",
    "\n",
    "The best part about embeddings is that are a *leanred* representation.\n",
    "\n",
    "eg. (in reality is very high in dimensions)\n",
    "```\n",
    "'a': 0\n",
    "'the': 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0adbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:general]",
   "language": "python",
   "name": "conda-env-general-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
